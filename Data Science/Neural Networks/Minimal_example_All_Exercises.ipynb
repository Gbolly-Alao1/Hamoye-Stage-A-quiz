{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression. Minimal example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the same code as before, please solve the following exercises\n",
    "    1. Change the number of observations to 100,000 and see what happens.\n",
    "    2. Change the number of observations to 1,000,000 and see what happens.\n",
    "    3. Play around with the learning rate. Values like 0.0001, 0.001, 0.1, 1 are all interesting to observe. \n",
    "    4. Change the loss function. L2-norm loss (without dividing by 2) is a good way to start. \n",
    "    5. Ð¢ry with the L1-norm loss, given by the sum of the ABSOLUTE value of yj - tj. The L1-norm loss is given by:\n",
    "## $$ \\Sigma_i = |y_i-t_i| $$\n",
    "    6. Create a function f(x,z) = 13*xs + 7*zs - 12. Does the algorithm work in the same way?\n",
    "    \n",
    "    \n",
    "Useful tip: When you change something, don't forget to RERUN all cells. This can be done easily by clicking:\n",
    "Kernel -> Restart & Run All\n",
    "If you don't do that, your algorithm will keep the OLD values of all parameters.\n",
    "\n",
    "You can either use this file for all the exercises, or check the solutions of EACH ONE of them in the separate files we have provided. All other files are solutions of each problem. If you feel confident enough, you can simply change values in this file. Please note that it will be nice, if you return the file to starting position after you have solved a problem, so you can use the lecture as a basis for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random input data to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n"
     ]
    }
   ],
   "source": [
    "# First, we should declare a variable containing the size of the training set we want to generate.\n",
    "observations = 100000\n",
    "\n",
    "# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.\n",
    "# We have picked x and z, since it is easier to differentiate them.\n",
    "# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).\n",
    "# The size of xs and zs is observations by 1. In this case: 1000 x 1.\n",
    "xs = np.random.uniform(low=-10, high=10, size=(observations,1))\n",
    "zs = np.random.uniform(-10, 10, (observations,1))\n",
    "\n",
    "# Combine the two dimensions of the input into one input matrix. \n",
    "# This is the X matrix from the linear model y = x*w + b.\n",
    "# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.\n",
    "inputs = np.column_stack((xs,zs))\n",
    "\n",
    "# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. \n",
    "# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.\n",
    "print (inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the targets we will aim at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 1)\n"
     ]
    }
   ],
   "source": [
    "# We want to \"make up\" a function, use the ML methodology, and see if the algorithm has learned it.\n",
    "# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>\n",
    "noise = np.random.uniform(-1, 1, (observations,1))\n",
    "\n",
    "# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.\n",
    "# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.\n",
    "targets = 2*xs - 3*zs + 5 + noise\n",
    "\n",
    "# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.\n",
    "print (targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training data\n",
    "The point is to see that there is a strong trend that our model should learn to reproduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14463461]\n",
      " [ 0.17093177]]\n",
      "[-0.05123711]\n"
     ]
    }
   ],
   "source": [
    "# We will initialize the weights and biases randomly in some small initial range.\n",
    "# init_range is the variable that will measure that.\n",
    "# You can play around with the initial range, but we don't really encourage you to do so.\n",
    "# High initial ranges may prevent the machine learning algorithm from learning.\n",
    "init_range = 0.2\n",
    "\n",
    "# Weights are of size k x m, where k is the number of input variables and m is the number of output variables\n",
    "# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)\n",
    "weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))\n",
    "\n",
    "# Biases are of size 1 since there is only 1 output. The bias is a scalar.\n",
    "biases = np.random.uniform(low=-init_range, high=init_range, size=1)\n",
    "\n",
    "#Print the weights to get a sense of how they were initialized.\n",
    "print (weights)\n",
    "print (biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some small learning rate (denoted eta in the lecture). \n",
    "# 0.02 is going to work quite well for our example. Once again, you can play around with it.\n",
    "# It is HIGHLY recommended that you play around with it.\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50422.74722436151\n",
      "50421.90655854388\n",
      "50421.0673739253\n",
      "50420.230486631306\n",
      "50419.39557098786\n",
      "50418.562443263254\n",
      "50417.73097050174\n",
      "50416.90067492345\n",
      "50416.07175596393\n",
      "50415.24457820875\n",
      "50414.41881257423\n",
      "50413.59455122793\n",
      "50412.77166451761\n",
      "50411.95029736012\n",
      "50411.13022524165\n",
      "50410.312338407806\n",
      "50409.49614230103\n",
      "50408.68164816505\n",
      "50407.86903515876\n",
      "50407.05790492681\n",
      "50406.248697330295\n",
      "50405.442096364815\n",
      "50404.63695003567\n",
      "50403.83388625427\n",
      "50403.03277879632\n",
      "50402.233156282025\n",
      "50401.43497540562\n",
      "50400.63804403913\n",
      "50399.84286110499\n",
      "50399.049200601396\n",
      "50398.25687913549\n",
      "50397.46615644928\n",
      "50396.67695962038\n",
      "50395.889393797224\n",
      "50395.10360984466\n",
      "50394.31921704244\n",
      "50393.536562762754\n",
      "50392.75512626361\n",
      "50391.974763001235\n",
      "50391.19586549196\n",
      "50390.417945906986\n",
      "50389.64132043146\n",
      "50388.86696200329\n",
      "50388.09488772618\n",
      "50387.32432701934\n",
      "50386.55520450827\n",
      "50385.78720974118\n",
      "50385.020490773386\n",
      "50384.25477725078\n",
      "50383.490765902\n",
      "50382.7284359684\n",
      "50381.96803867921\n",
      "50381.20958225722\n",
      "50380.45281789049\n",
      "50379.69805461032\n",
      "50378.94517517709\n",
      "50378.19385208778\n",
      "50377.44373083809\n",
      "50376.695155757334\n",
      "50375.94784521832\n",
      "50375.20168083443\n",
      "50374.456608065586\n",
      "50373.713327496676\n",
      "50372.971738271975\n",
      "50372.23177025473\n",
      "50371.49373049005\n",
      "50370.757116245906\n",
      "50370.02223308508\n",
      "50369.288425268925\n",
      "50368.55608087669\n",
      "50367.8252266117\n",
      "50367.09556568148\n",
      "50366.36738702591\n",
      "50365.64041587032\n",
      "50364.915077129524\n",
      "50364.19098805631\n",
      "50363.4679646543\n",
      "50362.74646876037\n",
      "50362.02660713647\n",
      "50361.307936243196\n",
      "50360.59080214688\n",
      "50359.87543244411\n",
      "50359.1618783246\n",
      "50358.44955369831\n",
      "50357.73867975045\n",
      "50357.0293987067\n",
      "50356.32130111061\n",
      "50355.61442254441\n",
      "50354.90885517354\n",
      "50354.204415782995\n",
      "50353.50128700223\n",
      "50352.79952843017\n",
      "50352.09877330592\n",
      "50351.399022929225\n",
      "50350.700503523265\n",
      "50350.00301899616\n",
      "50349.306789628754\n",
      "50348.611664704214\n",
      "50347.917596695224\n",
      "50347.225157344954\n",
      "50346.534889683164\n",
      "50345.84657614809\n",
      "50345.16014565538\n",
      "50344.475145045464\n",
      "50343.79201519084\n",
      "50343.11060997903\n",
      "50342.4306076271\n",
      "50341.75232824159\n",
      "50341.07542544263\n",
      "50340.40049940099\n",
      "50339.72713791919\n",
      "50339.054974237864\n",
      "50338.38411228503\n",
      "50337.714612388154\n",
      "50337.046462338665\n",
      "50336.37980542999\n",
      "50335.714606019894\n",
      "50335.050668983546\n",
      "50334.388324938416\n",
      "50333.7272626321\n",
      "50333.067409000614\n",
      "50332.40878884204\n",
      "50331.751222737985\n",
      "50331.094989339275\n",
      "50330.44022920535\n",
      "50329.786652022296\n",
      "50329.13462287758\n",
      "50328.48426533298\n",
      "50327.835450100014\n",
      "50327.18790797468\n",
      "50326.54156130263\n",
      "50325.89646525045\n",
      "50325.25231721235\n",
      "50324.60931726011\n",
      "50323.96763238128\n",
      "50323.32702137434\n",
      "50322.687930772954\n",
      "50322.050322204\n",
      "50321.41388975283\n",
      "50320.7784705607\n",
      "50320.14443816242\n",
      "50319.51177457503\n",
      "50318.88048912547\n",
      "50318.25036966337\n",
      "50317.62108719484\n",
      "50316.9928983358\n",
      "50316.36582670448\n",
      "50315.740060284006\n",
      "50315.115458254455\n",
      "50314.49243354391\n",
      "50313.87045473345\n",
      "50313.24940231354\n",
      "50312.62961842905\n",
      "50312.010836989655\n",
      "50311.3933552284\n",
      "50310.77714956843\n",
      "50310.161774318665\n",
      "50309.54810073195\n",
      "50308.93606993258\n",
      "50308.32541579165\n",
      "50307.71653589959\n",
      "50307.10972742097\n",
      "50306.50431826632\n",
      "50305.90020044358\n",
      "50305.29720918818\n",
      "50304.69549471083\n",
      "50304.09495586184\n",
      "50303.495714564815\n",
      "50302.897860014054\n",
      "50302.30104636949\n",
      "50301.705693519114\n",
      "50301.11159358073\n",
      "50300.518485638764\n",
      "50299.92634102271\n",
      "50299.335629822875\n",
      "50298.74649101014\n",
      "50298.159131381224\n",
      "50297.57317716725\n",
      "50296.988456422965\n",
      "50296.40481617465\n",
      "50295.822280141074\n",
      "50295.2408050589\n",
      "50294.660365273434\n",
      "50294.08081022161\n",
      "50293.50247656235\n",
      "50292.92559722296\n",
      "50292.34990639356\n",
      "50291.77537780144\n",
      "50291.202305736115\n",
      "50290.63016382339\n",
      "50290.05918135324\n",
      "50289.4892089937\n",
      "50288.92040821605\n",
      "50288.353010403545\n",
      "50287.78635445392\n",
      "50287.22054702912\n",
      "50286.655641960475\n",
      "50286.09217694444\n",
      "50285.52967528007\n",
      "50284.96813698012\n",
      "50284.40749771567\n",
      "50283.84798669914\n",
      "50283.28932712868\n",
      "50282.731890832474\n",
      "50282.175487868444\n",
      "50281.620202916245\n",
      "50281.065661943736\n",
      "50280.51235043621\n",
      "50279.960001916494\n",
      "50279.40888272748\n",
      "50278.85886396517\n",
      "50278.30998125165\n",
      "50277.76193122602\n",
      "50277.21499641096\n",
      "50276.66898241868\n",
      "50276.12404820488\n",
      "50275.580055288156\n",
      "50275.037072510146\n",
      "50274.495067559874\n",
      "50273.95417004128\n",
      "50273.41444693132\n",
      "50272.87569387823\n",
      "50272.337768756355\n",
      "50271.8006118537\n",
      "50271.2646207381\n",
      "50270.72949564755\n",
      "50270.19533496788\n",
      "50269.662445932365\n",
      "50269.13062535108\n",
      "50268.600002764426\n",
      "50268.07050067664\n",
      "50267.542227012935\n",
      "50267.015243852635\n",
      "50266.489570204336\n",
      "50265.96498931047\n",
      "50265.44174093611\n",
      "50264.91954762515\n",
      "50264.3979583416\n",
      "50263.87748070906\n",
      "50263.35775026923\n",
      "50262.838902021394\n",
      "50262.320883888155\n",
      "50261.80384860216\n",
      "50261.28811212398\n",
      "50260.773624745125\n",
      "50260.259940567885\n",
      "50259.74753469685\n",
      "50259.2360126918\n",
      "50258.72568754189\n",
      "50258.21633522482\n",
      "50257.70795595936\n",
      "50257.20072468855\n",
      "50256.694251928006\n",
      "50256.188414570446\n",
      "50255.68402151063\n",
      "50255.18062594473\n",
      "50254.678720842625\n",
      "50254.17841873807\n",
      "50253.679533394235\n",
      "50253.18152362317\n",
      "50252.68460281975\n",
      "50252.18852271203\n",
      "50251.69387622316\n",
      "50251.20056014519\n",
      "50250.70820837518\n",
      "50250.21685208865\n",
      "50249.726346919895\n",
      "50249.23711541569\n",
      "50248.74908475819\n",
      "50248.2619857583\n",
      "50247.77632798033\n",
      "50247.29146080978\n",
      "50246.80747835723\n",
      "50246.32449758593\n",
      "50245.8420865624\n",
      "50245.36034788251\n",
      "50244.87929139958\n",
      "50244.39886278049\n",
      "50243.919091198026\n",
      "50243.44017022242\n",
      "50242.96224585225\n",
      "50242.48551850114\n",
      "50242.00975370073\n",
      "50241.53462753139\n",
      "50241.06016355183\n",
      "50240.58663332793\n",
      "50240.11425992317\n",
      "50239.6426782217\n",
      "50239.17259574836\n",
      "50238.70336909419\n",
      "50238.23507807297\n",
      "50237.76775514693\n",
      "50237.30109879418\n",
      "50236.835429205035\n",
      "50236.370828451836\n",
      "50235.90729106305\n",
      "50235.44486933847\n",
      "50234.983798979956\n",
      "50234.52357311719\n",
      "50234.06461550778\n",
      "50233.60653891628\n",
      "50233.149128550365\n",
      "50232.69307344502\n",
      "50232.23789181323\n",
      "50231.783916913555\n",
      "50231.33074678909\n",
      "50230.87850444663\n",
      "50230.42754036772\n",
      "50229.97732567358\n",
      "50229.52806333418\n",
      "50229.079949007406\n",
      "50228.632759075736\n",
      "50228.18629494946\n",
      "50227.740931113745\n",
      "50227.296515956245\n",
      "50226.85276797449\n",
      "50226.40976607876\n",
      "50225.967581239034\n",
      "50225.52655566933\n",
      "50225.08633172089\n",
      "50224.64694804759\n",
      "50224.208493226935\n",
      "50223.77076848277\n",
      "50223.33378749878\n",
      "50222.89791713517\n",
      "50222.4631302872\n",
      "50222.029098800886\n",
      "50221.595901222165\n",
      "50221.16378642669\n",
      "50220.732813632814\n",
      "50220.30271131008\n",
      "50219.873715563626\n",
      "50219.44547584521\n",
      "50219.01802635324\n",
      "50218.59124094538\n",
      "50218.16556777185\n",
      "50217.740759997185\n",
      "50217.31705899949\n",
      "50216.89437906063\n",
      "50216.472525807236\n",
      "50216.0512734551\n",
      "50215.63068523638\n",
      "50215.21086595736\n",
      "50214.79180832235\n",
      "50214.37337292757\n",
      "50213.955824083656\n",
      "50213.53908519472\n",
      "50213.12333627162\n",
      "50212.708735121465\n",
      "50212.29498245791\n",
      "50211.88205023686\n",
      "50211.469855920404\n",
      "50211.058270806236\n",
      "50210.647386178694\n",
      "50210.23748428737\n",
      "50209.82838233132\n",
      "50209.41986208575\n",
      "50209.01228377289\n",
      "50208.605218116165\n",
      "50208.198698182285\n",
      "50207.79333998107\n",
      "50207.38876324512\n",
      "50206.98507339484\n",
      "50206.582190378496\n",
      "50206.18015799459\n",
      "50205.77899830876\n",
      "50205.37872307296\n",
      "50204.97920177134\n",
      "50204.580246627396\n",
      "50204.1819907193\n",
      "50203.784436988084\n",
      "50203.387937543666\n",
      "50202.99196869778\n",
      "50202.596701614886\n",
      "50202.202149722885\n",
      "50201.8083976055\n",
      "50201.415137163945\n",
      "50201.02261728323\n",
      "50200.63066885648\n",
      "50200.239543120704\n",
      "50199.84942020946\n",
      "50199.46001249739\n",
      "50199.07143803941\n",
      "50198.68371249199\n",
      "50198.296655909784\n",
      "50197.910311028136\n",
      "50197.524491854034\n",
      "50197.139671988974\n",
      "50196.75551193924\n",
      "50196.37232918219\n",
      "50195.98989760962\n",
      "50195.60808679747\n",
      "50195.226884447744\n",
      "50194.84624074148\n",
      "50194.46622578598\n",
      "50194.08683685353\n",
      "50193.70840139894\n",
      "50193.33094112302\n",
      "50192.954545604385\n",
      "50192.579362936034\n",
      "50192.20497026896\n",
      "50191.831205346374\n",
      "50191.458196859174\n",
      "50191.085988224666\n",
      "50190.71460230088\n",
      "50190.34391897812\n",
      "50189.97367074084\n",
      "50189.60420180521\n",
      "50189.23535455445\n",
      "50188.86719182632\n",
      "50188.49998347817\n",
      "50188.1336085341\n",
      "50187.76785372539\n",
      "50187.40273260301\n",
      "50187.038363556196\n",
      "50186.67499525371\n",
      "50186.31247596396\n",
      "50185.95063497927\n",
      "50185.58926857022\n",
      "50185.22845048796\n",
      "50184.868193144575\n",
      "50184.50856743495\n",
      "50184.14963168458\n",
      "50183.79135494929\n",
      "50183.43363316961\n",
      "50183.076883819966\n",
      "50182.721206855094\n",
      "50182.36623622969\n",
      "50182.01204096813\n",
      "50181.65859375502\n",
      "50181.30582013671\n",
      "50180.95377240479\n",
      "50180.602265058\n",
      "50180.2515162083\n",
      "50179.90133509245\n",
      "50179.551851423086\n",
      "50179.203073155404\n",
      "50178.85503952773\n",
      "50178.507942498334\n",
      "50178.16138971126\n",
      "50177.815620376816\n",
      "50177.47053672657\n",
      "50177.12630520313\n",
      "50176.782631039336\n",
      "50176.43945779594\n",
      "50176.09679454585\n",
      "50175.75476048125\n",
      "50175.41331450975\n",
      "50175.07247545847\n",
      "50174.732609807805\n",
      "50174.39386446427\n",
      "50174.05591386511\n",
      "50173.718710991314\n",
      "50173.38216836107\n",
      "50173.04606367214\n",
      "50172.71039591207\n",
      "50172.37539501087\n",
      "50172.04122771909\n",
      "50171.70774934913\n",
      "50171.3749750745\n",
      "50171.04282245775\n",
      "50170.71126321254\n",
      "50170.38013192447\n",
      "50170.049512348676\n",
      "50169.71955814869\n",
      "50169.39068289944\n",
      "50169.06246892481\n",
      "50168.734978843335\n",
      "50168.408145509275\n",
      "50168.08201176164\n",
      "50167.75629202869\n",
      "50167.431117474815\n",
      "50167.10657630149\n",
      "50166.78248755122\n",
      "50166.458982283235\n",
      "50166.13616632055\n",
      "50165.81400364611\n",
      "50165.492477731255\n",
      "50165.171496920004\n",
      "50164.851245289145\n",
      "50164.531746064255\n",
      "50164.212942684346\n",
      "50163.89504074601\n",
      "50163.577804770015\n",
      "50163.26109366189\n",
      "50162.944950978796\n",
      "50162.62925336006\n",
      "50162.314111236614\n",
      "50161.999527215245\n",
      "50161.6855415959\n",
      "50161.37220142518\n",
      "50161.05926655814\n",
      "50160.74682491316\n",
      "50160.43504866006\n",
      "50160.12400929879\n",
      "50159.81345936561\n",
      "50159.50367226687\n",
      "50159.19426960594\n",
      "50158.8853496087\n",
      "50158.57718794596\n",
      "50158.26947010801\n",
      "50157.96245419873\n",
      "50157.6562145702\n",
      "50157.350625777566\n",
      "50157.04567848246\n",
      "50156.74155812186\n",
      "50156.438004300704\n",
      "50156.1351601639\n",
      "50155.832950334145\n",
      "50155.53123794151\n",
      "50155.22993747939\n",
      "50154.9291712127\n",
      "50154.62891841508\n",
      "50154.329420994785\n",
      "50154.030499749875\n",
      "50153.73238392951\n",
      "50153.43482453381\n",
      "50153.13767283104\n",
      "50152.8410616057\n",
      "50152.54499684832\n",
      "50152.249461540116\n",
      "50151.95454450783\n",
      "50151.66020367238\n",
      "50151.36622814435\n",
      "50151.07291940217\n",
      "50150.7803446444\n",
      "50150.48831062317\n",
      "50150.19667867357\n",
      "50149.90558526783\n",
      "50149.614941442305\n",
      "50149.32485724076\n",
      "50149.035353895655\n",
      "50148.74665992679\n",
      "50148.45846635166\n",
      "50148.17091311814\n",
      "50147.88414885721\n",
      "50147.59803835362\n",
      "50147.312540821666\n",
      "50147.027405185014\n",
      "50146.7427746673\n",
      "50146.458608944464\n",
      "50146.17513056866\n",
      "50145.892310655094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50145.60999938084\n",
      "50145.32799546655\n",
      "50145.0464339321\n",
      "50144.76542470848\n",
      "50144.48502168878\n",
      "50144.205271995634\n",
      "50143.92613358214\n",
      "50143.64775071627\n",
      "50143.37031153142\n",
      "50143.093349769006\n",
      "50142.81692428179\n",
      "50142.540956118086\n",
      "50142.26565375169\n",
      "50141.99109464548\n",
      "50141.717040764575\n",
      "50141.443586534166\n",
      "50141.170665922946\n",
      "50140.8981366876\n",
      "50140.626076951434\n",
      "50140.354516813415\n",
      "50140.083508208816\n",
      "50139.8130595601\n",
      "50139.54319524935\n",
      "50139.273856589694\n",
      "50139.00514881334\n",
      "50138.7369442382\n",
      "50138.46926851005\n",
      "50138.20219718783\n",
      "50137.93542106937\n",
      "50137.66899297792\n",
      "50137.40313909417\n",
      "50137.13774071371\n",
      "50136.87279436007\n",
      "50136.608293718964\n",
      "50136.34451457532\n",
      "50136.08125448722\n",
      "50135.81841015464\n",
      "50135.55610142859\n",
      "50135.294891340454\n",
      "50135.03423128954\n",
      "50134.77401810226\n",
      "50134.51453597852\n",
      "50134.255628861814\n",
      "50133.99726811369\n",
      "50133.73938360461\n",
      "50133.48197188598\n",
      "50133.22522366257\n",
      "50132.96906089521\n",
      "50132.713424827736\n",
      "50132.45838321343\n",
      "50132.20374270955\n",
      "50131.9494488817\n",
      "50131.69570302947\n",
      "50131.44253877906\n",
      "50131.189773129096\n",
      "50130.93742132292\n",
      "50130.68553586334\n",
      "50130.43440967005\n",
      "50130.184067908485\n",
      "50129.93425386298\n",
      "50129.68497573908\n",
      "50129.435988599755\n",
      "50129.18742839954\n",
      "50128.93934024325\n",
      "50128.691828590054\n",
      "50128.44460311473\n",
      "50128.197863206944\n",
      "50127.95195469919\n",
      "50127.706768183714\n",
      "50127.4622279673\n",
      "50127.218206522295\n",
      "50126.974772960915\n",
      "50126.73200122234\n",
      "50126.489634604404\n",
      "50126.24758611132\n",
      "50126.00581492303\n",
      "50125.7645063497\n",
      "50125.52383494451\n",
      "50125.28365706728\n",
      "50125.044164297775\n",
      "50124.80521021216\n",
      "50124.566554060104\n",
      "50124.32840607238\n",
      "50124.09099666698\n",
      "50123.85409333219\n",
      "50123.61748649504\n",
      "50123.38134634405\n",
      "50123.14570952798\n",
      "50122.9104075688\n",
      "50122.67542235842\n",
      "50122.44067781785\n",
      "50122.20643434867\n",
      "50121.97257685281\n",
      "50121.739142994724\n",
      "50121.506106750276\n",
      "50121.27335395129\n",
      "50121.04104135428\n",
      "50120.8091943214\n",
      "50120.577716734515\n",
      "50120.346733337785\n",
      "50120.116070632954\n",
      "50119.88580854161\n",
      "50119.65604077974\n",
      "50119.42714377886\n",
      "50119.19875115771\n",
      "50118.97070971956\n",
      "50118.74301096209\n",
      "50118.5157164696\n",
      "50118.28875538022\n",
      "50118.0622480559\n",
      "50117.83622848779\n",
      "50117.61079421963\n",
      "50117.38569842111\n",
      "50117.16094690264\n",
      "50116.93673842231\n",
      "50116.712907680136\n",
      "50116.48952568999\n",
      "50116.26637735847\n",
      "50116.04360787654\n",
      "50115.82131146223\n",
      "50115.599345076895\n",
      "50115.377757320675\n",
      "50115.15667054579\n",
      "50114.93615385148\n",
      "50114.71612121734\n",
      "50114.496637680924\n",
      "50114.277542975215\n",
      "50114.05869326427\n",
      "50113.8402248516\n",
      "50113.62218552281\n",
      "50113.40440996659\n",
      "50113.18710090074\n",
      "50112.9701781233\n",
      "50112.75394813206\n",
      "50112.53834060909\n",
      "50112.32314252657\n",
      "50112.10839913229\n",
      "50111.893969167206\n",
      "50111.67999964251\n",
      "50111.4665045353\n",
      "50111.25345184556\n",
      "50111.040780738986\n",
      "50110.828563892734\n",
      "50110.616908343334\n",
      "50110.40577712666\n",
      "50110.19509217551\n",
      "50109.9847553149\n",
      "50109.7748197223\n",
      "50109.56527579801\n",
      "50109.356047387635\n",
      "50109.14729395781\n",
      "50108.9393020937\n",
      "50108.731863584144\n",
      "50108.5248931524\n",
      "50108.31838105177\n",
      "50108.11212773433\n",
      "50107.90626454436\n",
      "50107.700909741565\n",
      "50107.496244874106\n",
      "50107.29212524851\n",
      "50107.088509665904\n",
      "50106.885332656646\n",
      "50106.68260474083\n",
      "50106.48021985578\n",
      "50106.27817339463\n",
      "50106.07656307655\n",
      "50105.87556904399\n",
      "50105.67512520144\n",
      "50105.47526434815\n",
      "50105.27612747764\n",
      "50105.077545773354\n",
      "50104.87944041345\n",
      "50104.681878661664\n",
      "50104.48468721936\n",
      "50104.28783966429\n",
      "50104.09131507877\n",
      "50103.895087509794\n",
      "50103.69920537289\n",
      "50103.50397519939\n",
      "50103.30929494807\n",
      "50103.11510016347\n",
      "50102.921388338924\n",
      "50102.72799971964\n",
      "50102.53508983616\n",
      "50102.34256966892\n",
      "50102.15055958632\n",
      "50101.95889189192\n",
      "50101.767507063705\n",
      "50101.57647557696\n",
      "50101.385743999126\n",
      "50101.19535876292\n",
      "50101.005577373246\n",
      "50100.81636049844\n",
      "50100.62757412441\n",
      "50100.43903308503\n",
      "50100.2507315523\n",
      "50100.062816684265\n",
      "50099.87532081884\n",
      "50099.68814802941\n",
      "50099.50129992654\n",
      "50099.31478453221\n",
      "50099.12855869264\n",
      "50098.94282299378\n",
      "50098.75741505462\n",
      "50098.57233658569\n",
      "50098.3877248117\n",
      "50098.20365208774\n",
      "50098.019902523774\n",
      "50097.8367296935\n",
      "50097.65402485152\n",
      "50097.4717943338\n",
      "50097.28991341642\n",
      "50097.10835157727\n",
      "50096.92715927865\n",
      "50096.746305992085\n",
      "50096.56578077683\n",
      "50096.3856722344\n",
      "50096.20587210815\n",
      "50096.02653825717\n",
      "50095.847611910445\n",
      "50095.6690206838\n",
      "50095.49062338262\n",
      "50095.312503880516\n",
      "50095.13473928209\n",
      "50094.95725815143\n",
      "50094.78024019583\n",
      "50094.60380480805\n",
      "50094.42781323794\n",
      "50094.25212641091\n",
      "50094.07682882926\n",
      "50093.90198190534\n",
      "50093.72739896657\n",
      "50093.55299749547\n",
      "50093.37898066902\n",
      "50093.20542244256\n",
      "50093.032218172455\n",
      "50092.85925349312\n",
      "50092.68651495933\n",
      "50092.51399977632\n",
      "50092.34174440432\n",
      "50092.16978566654\n",
      "50091.998075580246\n",
      "50091.82660209306\n",
      "50091.65549198015\n",
      "50091.484750516654\n",
      "50091.31442325967\n",
      "50091.14441912235\n",
      "50090.974691828924\n",
      "50090.80521027888\n",
      "50090.6362049381\n",
      "50090.46742862835\n",
      "50090.29893715279\n",
      "50090.130690666934\n",
      "50089.96280403592\n",
      "50089.79514331955\n",
      "50089.62777252535\n",
      "50089.460841761975\n",
      "50089.294282803465\n",
      "50089.12815720265\n",
      "50088.96229505221\n",
      "50088.79672468737\n",
      "50088.63140513347\n",
      "50088.466514332125\n",
      "50088.3019931494\n",
      "50088.137785382285\n",
      "50087.97398308214\n",
      "50087.8104250336\n",
      "50087.647074826345\n",
      "50087.48400065333\n",
      "50087.32130370962\n",
      "50087.15913429894\n",
      "50086.99729025763\n",
      "50086.835632924136\n",
      "50086.674193858256\n",
      "50086.51298936497\n",
      "50086.35195413321\n",
      "50086.19113178867\n",
      "50086.03073901088\n",
      "50085.87083968086\n",
      "50085.71129271589\n",
      "50085.55209859815\n",
      "50085.393210634684\n",
      "50085.234596946386\n",
      "50085.076265395954\n",
      "50084.91827018597\n",
      "50084.76063386527\n",
      "50084.603265178055\n",
      "50084.446243452\n",
      "50084.28952905167\n",
      "50084.133129541544\n",
      "50083.976903277035\n",
      "50083.82090959978\n",
      "50083.6652970774\n",
      "50083.509961328484\n",
      "50083.35491850973\n",
      "50083.20010939514\n",
      "50083.045455083804\n",
      "50082.89103349122\n",
      "50082.73684748803\n",
      "50082.58299307481\n",
      "50082.42959275856\n",
      "50082.27669457853\n",
      "50082.12407487028\n",
      "50081.971613635964\n",
      "50081.81948988804\n",
      "50081.66764469068\n",
      "50081.51599805615\n",
      "50081.364614378734\n",
      "50081.2134754001\n",
      "50081.062623592865\n",
      "50080.911979386095\n",
      "50080.76155505863\n",
      "50080.611381177165\n",
      "50080.461563705554\n",
      "50080.31208185526\n",
      "50080.162970871235\n",
      "50080.01413903936\n",
      "50079.86558557032\n",
      "50079.71728952965\n",
      "50079.56925967871\n",
      "50079.42152096446\n",
      "50079.27410296638\n",
      "50079.12725334118\n",
      "50078.980688889606\n",
      "50078.83434389015\n",
      "50078.68822024824\n",
      "50078.5422885187\n",
      "50078.39661398635\n",
      "50078.251159966145\n",
      "50078.10592458939\n",
      "50077.960992466134\n",
      "50077.816238730746\n",
      "50077.671866672325\n",
      "50077.527775679155\n",
      "50077.38389074407\n",
      "50077.24022028598\n",
      "50077.09682236286\n",
      "50076.95359174758\n",
      "50076.81057772195\n",
      "50076.66771286341\n",
      "50076.52506433382\n",
      "50076.38272810377\n",
      "50076.24064611064\n",
      "50076.09901847968\n",
      "50075.95764133574\n",
      "50075.81662692155\n",
      "50075.67593294092\n",
      "50075.53558187902\n",
      "50075.39549639133\n",
      "50075.25562088819\n",
      "50075.116003390714\n",
      "50074.97679832717\n",
      "50074.83795023185\n",
      "50074.69925219558\n",
      "50074.56085300712\n",
      "50074.42287797258\n",
      "50074.28529158313\n",
      "50074.148098506856\n",
      "50074.01120326384\n",
      "50073.87445668978\n",
      "50073.73787290883\n",
      "50073.60147364483\n",
      "50073.46533447766\n",
      "50073.32957272127\n",
      "50073.1941899076\n",
      "50073.05906480111\n",
      "50072.92415660764\n",
      "50072.78948888065\n",
      "50072.655038154626\n",
      "50072.520909188606\n",
      "50072.3870364741\n",
      "50072.25336013386\n",
      "50072.11986730393\n",
      "50071.98665626824\n",
      "50071.8537679575\n",
      "50071.72110051465\n",
      "50071.58872255494\n",
      "50071.45649327883\n",
      "50071.32456008623\n",
      "50071.192971938624\n",
      "50071.061691871815\n",
      "50070.9308822793\n",
      "50070.800473386735\n",
      "50070.6704413348\n",
      "50070.54058288193\n",
      "50070.41094354683\n",
      "50070.28149095774\n",
      "50070.152167816326\n",
      "50070.02299631886\n",
      "50069.894049894916\n",
      "50069.765363326485\n",
      "50069.636922657905\n",
      "50069.5087665115\n",
      "50069.38082487954\n",
      "50069.25305630791\n",
      "50069.12549610202\n",
      "50068.99815645202\n",
      "50068.87113906701\n",
      "50068.74441953716\n",
      "50068.61797889904\n",
      "50068.491898335276\n",
      "50068.365955651185\n",
      "50068.240195725186\n",
      "50068.114734236486\n",
      "50067.98955420031\n",
      "50067.86464971769\n",
      "50067.74014524936\n",
      "50067.61591332506\n",
      "50067.49193918155\n",
      "50067.368249751904\n",
      "50067.244841700216\n",
      "50067.121758588684\n",
      "50066.998943241095\n",
      "50066.876438301304\n",
      "50066.75419321342\n",
      "50066.63217956542\n",
      "50066.51054865514\n",
      "50066.38927255934\n",
      "50066.26832023026\n",
      "50066.14759052743\n",
      "50066.02701811429\n",
      "50065.90667268199\n",
      "50065.786682620754\n",
      "50065.66699531802\n",
      "50065.54742769804\n",
      "50065.42801447716\n",
      "50065.30881138593\n",
      "50065.18983539291\n",
      "50065.071108223776\n",
      "50064.95255657047\n",
      "50064.83419652129\n",
      "50064.71608328493\n",
      "50064.598267527654\n",
      "50064.48063866304\n",
      "50064.363221526226\n",
      "50064.24593699922\n",
      "50064.128827732224\n",
      "50064.01210400292\n",
      "50063.89560829748\n",
      "50063.77943172072\n",
      "50063.66351684138\n",
      "50063.54782881089\n",
      "50063.4323407151\n",
      "50063.31711861608\n",
      "50063.20216800844\n",
      "50063.087530310106\n",
      "50062.97312270827\n",
      "50062.85888200199\n",
      "50062.74484232559\n",
      "50062.63096694378\n",
      "50062.51725159839\n",
      "50062.40368488045\n",
      "50062.29030182647\n",
      "50062.17712846826\n",
      "50062.06417267877\n",
      "50061.95138750753\n",
      "50061.8387971423\n"
     ]
    }
   ],
   "source": [
    "# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.\n",
    "# The proper number of iterations is something we will talk about later on, but generally\n",
    "# a lower learning rate would need more iterations, while a higher learning rate would need less iterations\n",
    "# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.\n",
    "for i in range (1000):\n",
    "    \n",
    "    # This is the linear model: y = xw + b equation\n",
    "    outputs = np.dot(inputs,weights) + biases\n",
    "    # The deltas are the differences between the outputs and the targets\n",
    "    # Note that deltas here is a vector 1000 x 1\n",
    "    deltas = outputs - targets\n",
    "        \n",
    "    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.\n",
    "    # Moreover, we further divide it by the number of observations.\n",
    "    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,\n",
    "    # as any function holding the basic property of being lower for better results, and higher for worse results\n",
    "    # can be a loss function.\n",
    "    loss = np.sum(abs(deltas)) \n",
    "    \n",
    "    # We print the loss function value at each step so we can observe whether it is decreasing as desired.\n",
    "    print (loss)\n",
    "    \n",
    "    # Another small trick is to scale the deltas the same way as the loss function\n",
    "    # In this way our learning rate is independent of the number of samples (observations).\n",
    "    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate\n",
    "    # that can remain the same if we change the number of training samples (observations).\n",
    "    # You can try solving the problem without rescaling to see how that works for you.\n",
    "    deltas_scaled = deltas / observations\n",
    "    \n",
    "    # Finally, we must apply the gradient descent update rules from the relevant lecture.\n",
    "    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1\n",
    "    # We must transpose the inputs so that we get an allowed operation.\n",
    "    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)\n",
    "    biases = biases - learning_rate * np.sum(deltas_scaled)\n",
    "    \n",
    "    # The weights are updated in a linear algebraic way (a matrix minus another matrix)\n",
    "    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.\n",
    "    # The two lines are both consistent with the gradient descent methodology. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print weights and biases and see if we have worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.00006779]\n",
      " [-3.00010388]] [4.96517475]\n"
     ]
    }
   ],
   "source": [
    "# We print the weights and the biases, so we can see if they have converged to what we wanted.\n",
    "# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.\n",
    "print (weights, biases)\n",
    "\n",
    "# Note that they may be convergING. So more iterations are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot last outputs vs targets\n",
    "Since they are the last ones at the end of the training, they represent the final model accuracy. <br/>\n",
    "The closer this plot is to a 45 degree line, the closer target and output values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We print the outputs and the targets in order to see if they have a linear relationship.\n",
    "# Again, that's not needed. Moreover, in later lectures, that would not even be possible.\n",
    "plt.plot(outputs,targets)\n",
    "plt.xlabel('outputs')\n",
    "plt.ylabel('targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
